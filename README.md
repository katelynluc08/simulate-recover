The ez diffusion model's consistency performance is tested upon with taking the overall average error and comparing it to a set threshold value used to determine whether the estimated parameters that the model outputted are close enough to the original parameters. The overall average error is essentially the mean accumulation of all 3000 average errors across all sample sizes. I set the threshold to 0.4 because 0.5 is the half point dividing whether a bias difference is either large or small. 0.4 sits right below the half point so I decided anything lower than 0.4 would be considered a small enough difference between the original parameters and estimated parameters to say it is similar or close enough. Any value that is higher is too big of a bias or error. I also wanted to check if this model outputs bias average per sample size as close enough to 0 to identify if there are any biases that the model leans more towards (overestimation or undersestimation). Furthermore, see if the average bias squared would decrease as sample size increases. This checks if the model gets more accurate with increasing sample size, which accompanies less noise. 

First off let's talk about the total average error. I got a average error of 0.705 across all the sample sizes. Since this is above the man-made threshold I set, I conclude in my standards that this model is not consistent as we would like it to be across all sample sizes. This suggests that the parameter estimates that the model produces is not sufficiently accurate. Even without my threshold, I feel that this value is leaning towards 1.0 so it is a high average error (bias) that the model produces with each estimate for the sample sizes given. 

I tracked how the squared error changed as the sample size increased from 10 to 4000. The results can be examined in the 'Results.txt' file src directory of my repository. The sample size of 10 had a squared error of 2.44, the sample size of 40 has squared error of 1.31, and the sample size of 4000 had a squared error of 0.26. As you can examine, the squared error across sample sizes decreased from 2.44 to 0.26 as the sample size increased from 10 to 4000. This allows us to confirm that as N increases in size, the b^2 decreases. So our model was able to become more accurate with more sample size because of less noise. Larger sample sizes breed better accuracy with the average of parameter estimates. 

I also tracked average bias for each parameter per sample size to see if the bias is consistently averaged to 0. The results can be examined in the 'Results.txt' file src directory of my repository.This allows us to see how accurate our model was in calculating the estimated parameters compared to the original parameters. For a sample size of 10: the v(drift rate) = -1.63474, a(boundary separation) = 0.01768, t(non-decision time) = 0.20988. For a sample size of 40: the v(drift rate) = -0.71859, a(boundary separation) = -0.00021, t(non-decision time) = 0.23317. For a sample size of 4000: the v(drift rate) = -0.47095, a(boundary separation) = 0.07252, t(non-decision time) = 0.04660. My interpretion of the biases: in sample size 10, the bias for v was underestimated by the model as shown with the large negative bias, meaning that the estimated V value is lower than the actual V value on average. The bias for a has a small positive bias, indicating that the model is almost accurately estimating the boundary separation, but just slightly overestimating it a little. The t is positive bias that is relatively large to 0, which suggests that the model is overestimating the non-decision time for small sample sizes. As the sample sizes increase to 40 and 4000, the biases overall decrease, especially for drift rate. It approaches 0 as you increase sample size. Though, the model struggles with computing non-decision time accuracy by overestimating it still, however, the bias reduces with larger sample sizes significantly (e.g. t_value bias for 4000 size). The boundary separation parameter is the most consistent with smaller biases as sample size increases. It seems that for smaller sample sizes, the biases are larger especially for drift rate and non-decision time. This suggests that the model needs more data to get better accurate estimates for these parameters. 

Looking across all samples, the model tends to underestimate the drift rate as seen with the negative bias, but as samples increase it gets better at estimating the parameter. The boundary separation bias is small and positive consistently, indicating that the model does slightly overestimate it but it is not significant enough. Lastly, since non-decision bias is consistently positive, it highlights that the model is overestimating the parameter. However, this bias does decrease as sample size increases. 

Overall, the model's consistency is not as sufficiently accurate to how much we would like it to be. The model fails to meet my consistency threshold of 0.4 for the average error across sample sizes. However, the consistency does improve as sample sizes increase, as shown with the smaller squared errors and biases for larger sample sizes. Which is why I concluded it is not sufficiently accurate. Sample sizes do play a role in how biased the model will be, with smaller sample sizes, there is more bias, but larger sample sizes, there is less bias. The reliability of the model improves with larger data, but for smaller, it gives a decent amount of bias. 
